using Statistics

"""
Transformers enable you to transform the output generated by a datasets.
Many of the transformers can be chained together.

```julia
data = SomeDataset(...)
data = data |> Transformer1(...) |> Transformer2(...) |> Transformer3(...)

train!(workout, data)
```
"""
abstract type Transformer <: Dataset end

Base.length(t::Transformer) = length(t.ds)
function (t::Transformer)(ds::Dataset)
    t.ds = ds
    return t
end


"""
Add noise to the samples in a dataset. By default will only add noise to the
input data (:X) and not the labels (:Y).

# Usage

```julia
ds = TestDataset((28,28,1),(10,),100)
ds = ds |> NoiseTransformer(noiselevel=0.05)
```
"""
mutable struct NoisingTransfomer <: Transformer
    ds::Union{Dataset, Nothing}
	noiselevel
	axis

    NoisingTransfomer(noiselevel=0.01; axis=[:X]) = new(nothing,noiselevel,axis)
end

genNoise(arr::AbstractArray, level::Number) = randn(eltype(arr), size(arr)) .* level

function Base.getindex(t::NoisingTransfomer, idx)
	X,Y = t.ds[idx]
	if :X in t.axis
		X += genNoise(X, t.noiselevel)
	end

	if :Y in t.axis
		Y += genNoise(Y, t.noiselevel)
	end

	return (X,Y)
end


"""
Normalize data before feeding it to a model. A typical usecase is to normalize
an image.  By default will only normalize to the input data (:X) and not the labels (:Y).

# Usage

```julia
ds = TestDataset((28,28,1),(10,),100) |> Normalize()
```
"""
mutable struct Normalizer <: Transformer
    ds::Union{Dataset, Nothing}
	dims::Array{Int}
	means
	stds
	axis

    Normalizer(means=0, stds=1, dims=[1,2]; axis=[:X]) =
		new(nothing,dims, means, stds, axis)
end

function normalize(x, means, stds)
	(x .- means) ./ stds
end

function Base.getindex(t::Normalizer, idx)
	X,Y = t.ds[idx]
	if :X in t.axis
		X += normalize(X, t.means, t.stds)
	end

	if :Y in t.axis
		Y += normalize(Y, t.means, t.stds)
	end

	return (X,Y)
end


mutable struct Standardizer <: Transformer
    ds::Union{Dataset, Nothing}
	mean
	std
    Standardize() = new(nothing, 0.0f0, 1.0f0)
end

function fit!(t::Standardizer, x)
	t.mean = mean(x)
	t.std = std(x)
end

function Base.getindex(t::Standardizer, idx)
	X,Y = t.ds[idx]

	Y = normalize(Y, t.mean, t.std)

	return (X,Y)
end



"""
Randomly crop an image in the shape of WxHxC to a certain size. This Transformer
uses plain Array operations and doesn't rely on packages like Images.jl

# Usage

```julia
# Only crop X
ds = TestDataset((100,100,3),(10,),100)
ds = ds |> ImageCrop((50,50))


# Crop both X and Y
ds = TestDataset((100,100,3),(50,50,3),100)
ds = ds |> ImageCrop((50,50), (20,20))
```

"""
mutable struct ImageCrop <: Transformer
    ds::Union{Dataset, Nothing}
	shapeX
	shapeY

    ImageCrop(shapeX , shapeY=nothing) = new(nothing,shapeX, shapeY)
end

function imageCrop(arr::AbstractArray,shape)
	maxX = size(arr,1) - shape[1]
	maxY = size(arr,2) - shape[2]
	offX = rand(1:maxX)
	offY = rand(1:maxY)
	arr[offX:offX+shape[1]-1,offY:offY+shape[2]-1, :]
end


function Base.getindex(t::ImageCrop, idx)
	X,Y = t.ds[idx]
	if t.shapeX !== nothing
		X = imageCrop(X, t.shapeX)
	end

	if t.shapeY !== nothing
		Y = imageCrop(Y, t.shapeY)
	end

	return (X,Y)
end



function onehot(x, labels, dtype::Type)
	result = zeros(dtype, length(labels))
    result[findfirst(x .== labels)] = 1
    result
end


function onehot(X::AbstractArray, labels, dtype::Type)
	result = zeros(dtype, length(labels))
	for x in X
		result[findfirst(x .== labels)] = 1
	end
	result
end


"""
OneHot transformer for a single sample. Uses
the onehot function to perform the actual transoformation.

Supports both single and multi class encoding:

	5 => [0,0,0,0,0,1,0,0,0,0]
	[5,7] => [0,0,0,0,0,1,0,1,0,0]

# Usage

```julia
ds = mnistds() |> OneHotEncoder(0:9)
```
"""
mutable struct OneHotEncoder <: Transformer
    ds::Union{Dataset, Nothing}
	labels
	axis
	dtype::Type

    OneHotEncoder(labels; axis=[:Y], dtype=getcontext().dtype) =
		new(nothing, labels, axis, dtype)
end


function Base.getindex(t::OneHotEncoder, idx)
	X,Y = t.ds[idx]
	if :X in t.axis
		X = onehot(X, t.labels, t.dtype)
	end

	if :Y in t.axis
		Y = onehot(Y, t.labels, t.dtype)
	end

	return (X,Y)
end

"""
Return a subset of samples of the underlying dataset. Optionally
you can specify a start index (default = 1).

# Usage

```julia
data = MyDataset() |> Subset(1:100)
```
"""
mutable struct Subset <: Transformer
    ds::Union{Dataset, Nothing}
	idxs::AbstractArray{Int}

    Subset(idxs::AbstractArray{Int}) = new(nothing, idxs)
end

Base.length(t::Subset) = length(t.idxs)
Base.getindex(t::Subset, idx) = t.ds[t.idxs[idx]]


"""
Split one dataset into two subsets, one for training and one for validation.
By default Split will shuffle the indexes first in order to get a representative
validaiton set. The default split is 80% for training and 20% for validation.

# Usage

```julia
data_train, data_valid = ImageDataset(...) |> Split(0.25, shuffle=false)
```
"""
mutable struct Split <: Transformer
    ds::Union{Dataset, Nothing}
	valid_perc::Float64
	shuffle::Bool

    Split(valid_perc=0.2; shuffle=true) =
		new(nothing, valid_perc, shuffle)
end

function (t::Split)(ds::Dataset)
    t.ds = ds
	maxl = length(ds)

	idxs = t.shuffle ? Random.shuffle(1:maxl) : 1:maxl
	cut = round(Int, maxl*(1.0-t.valid_perc))

    a, b = (Subset(idxs[1:cut]), Subset(idxs[cut+1:end]))
	return (a(ds), b(ds))
end


#=
# MiniBatch helper functions
=#
function update_mb!(arr::AbstractArray, elem::AbstractArray, idx::Int)
	@assert size(arr)[1:end-1] == size(elem) "$(size(arr)) $(size(elem))"
	idxs = Base.OneTo.(size(elem))
	arr[idxs..., idx] = elem
end


function update_mb!(t::Tuple, elems::Tuple, idx::Int)
	@assert length(t) == length(elems)
	for (arr,elem) in zip(t,elems)
		update_mb!(arr, elem, idx)
	end
end

# Create a minibatch that has a similar size to the structure returned
# from the dataset.
create_mb(arr::AbstractArray, batchsize) = similar(arr, size(arr)..., batchsize)
create_mb(t::Tuple, batchsize)= Tuple(collect(create_mb(elem, batchsize) for elem in t))

"""
A MiniBatch is a transformer that will collect multiple samples from the underlying
dataset and put it in a minibatch. So it will call a dataset to get a single sample
and combines them into a minibatch. The MiniBatch itself is an iterator.

It will do this using threading, so when a dataset will read some samples from disk
this won't become a botttleneck.

# Usage

```julia
data = ImageDataset(filenames,labels) |> MiniBatch(16, shuffle=False)
train!(workout, data)
```

"""
mutable struct MiniBatch <: Transformer
    ds::Union{Dataset, Nothing}
    batchsize::Int
    shuffle::Bool

    MiniBatch(batchsize=8; shuffle=true) =
        new(nothing, batchsize, shuffle)
end

Base.length(dl::MiniBatch) = length(dl.ds) รท dl.batchsize

function Base.iterate(dl::MiniBatch, state=undef)
    maxl = length(dl.ds)
    bs = dl.batchsize

    if state == undef
        idxs = dl.shuffle ? Random.shuffle(1:maxl) : 1:maxl
        state = (idxs,1)
    end
    idxs, count = state

    if count > (maxl-bs) return nothing end

	l = ReentrantLock()
	minibatch = nothing

    Threads.@threads for i in 1:bs

		idx = i + count - 1
		sample = dl.ds[idx]
		@assert sample isa Tuple "Datasets should return Tuples, not $(typeof(sample))"

		if minibatch === nothing
			lock(l) do
				if minibatch === nothing
					minibatch = create_mb(sample, bs)
				end
			end
		end

		update_mb!(minibatch, sample, i)
    end
    return ((minibatch), (idxs, count + bs))
end
